{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wira.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDaDh7oHH0h5"
      },
      "source": [
        "https://medium.com/dataseries/how-to-scrape-millions-of-tweets-using-snscrape-195ee3594721"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_ILaCWv-qk5"
      },
      "source": [
        "## SENTIMENT ANALYSIS\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIVLvPaez1Hj",
        "outputId": "8c003447-fb39-428b-b40c-1674a642d56b"
      },
      "source": [
        "pip install textblob"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2_G1Ooxz1L6"
      },
      "source": [
        "from textblob import TextBlob\n",
        "#blob = TextBlob(df['Clean'][2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRg754EZrs-R"
      },
      "source": [
        "df['Sentiment_textblob'] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdjV_PjYkvtR",
        "outputId": "f5f4ef2a-47ae-4b27-ca64-80cbcf928dd7"
      },
      "source": [
        "gme3['Sentiment_textblob'] = 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twgkjTLvlCfk"
      },
      "source": [
        "gme3 = gme3.reset_index().drop(columns = ['index'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtZcmH7qkzJy"
      },
      "source": [
        "for i in range(gme3.shape[0]):\n",
        "  blob = TextBlob(gme3['content'][i])\n",
        "  gme3.iloc[i,2] = blob.sentiment.polarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vf-iluiP4NA9",
        "outputId": "32648cc6-b376-47e2-94ef-4790817078aa"
      },
      "source": [
        "pip install vaderSentiment"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 22.1 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 20 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 40 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51 kB 2.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 61 kB 2.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 71 kB 2.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 81 kB 2.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 92 kB 2.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 102 kB 2.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 112 kB 2.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 122 kB 2.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 125 kB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vaderSentiment) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkgRoi8h4N93"
      },
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrXJE7yLlt-5"
      },
      "source": [
        "gme3['Sentiment_vader'] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aF6DKNGWlOM3"
      },
      "source": [
        "sid_obj = SentimentIntensityAnalyzer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uH4Moc1MlOgM"
      },
      "source": [
        "for i in range(gme3.shape[0]):\n",
        "  sentence = gme3['content'][i]\n",
        "  sentiment_dict = sid_obj.polarity_scores(sentence)\n",
        "  gme3.iloc[i,3] = sentiment_dict['compound']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "ZvGQAkOWlToY",
        "outputId": "547372f9-ac93-4a57-de5f-2f3702bfddc7"
      },
      "source": [
        "gme3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>content</th>\n",
              "      <th>Sentiment_textblob</th>\n",
              "      <th>Sentiment_vader</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-09-18 23:56:45+00:00</td>\n",
              "      <td>confirms drs protect share phantom share put d...</td>\n",
              "      <td>-0.200000</td>\n",
              "      <td>0.7717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2021-09-18 23:53:47+00:00</td>\n",
              "      <td>aggressivevolatile high short squeeze stock po...</td>\n",
              "      <td>0.032000</td>\n",
              "      <td>-0.3182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2021-09-18 23:50:11+00:00</td>\n",
              "      <td>spacexs private inspiration4 mission splash sa...</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.4939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2021-09-18 23:50:00+00:00</td>\n",
              "      <td>alert run gain daily alert one time fee beat t...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.7783</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2021-09-18 23:48:38+00:00</td>\n",
              "      <td>stop</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.2960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43354</th>\n",
              "      <td>2021-08-19 00:02:08+00:00</td>\n",
              "      <td>almost half like good tweet date would funny b...</td>\n",
              "      <td>0.261111</td>\n",
              "      <td>0.7755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43355</th>\n",
              "      <td>2021-08-19 00:01:58+00:00</td>\n",
              "      <td>why fuck company like hold work person fund el...</td>\n",
              "      <td>-0.350000</td>\n",
              "      <td>-0.2500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43356</th>\n",
              "      <td>2021-08-19 00:00:38+00:00</td>\n",
              "      <td>stock crater that cost</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43357</th>\n",
              "      <td>2021-08-19 00:00:31+00:00</td>\n",
              "      <td>wait coworkers moass specially boss make fun a...</td>\n",
              "      <td>0.319048</td>\n",
              "      <td>0.7845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43358</th>\n",
              "      <td>2021-08-19 00:00:01+00:00</td>\n",
              "      <td>alert run gain daily alert one time fee beat t...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.7783</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>43359 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                            date  ... Sentiment_vader\n",
              "0      2021-09-18 23:56:45+00:00  ...          0.7717\n",
              "1      2021-09-18 23:53:47+00:00  ...         -0.3182\n",
              "2      2021-09-18 23:50:11+00:00  ...          0.4939\n",
              "3      2021-09-18 23:50:00+00:00  ...          0.7783\n",
              "4      2021-09-18 23:48:38+00:00  ...         -0.2960\n",
              "...                          ...  ...             ...\n",
              "43354  2021-08-19 00:02:08+00:00  ...          0.7755\n",
              "43355  2021-08-19 00:01:58+00:00  ...         -0.2500\n",
              "43356  2021-08-19 00:00:38+00:00  ...          0.0000\n",
              "43357  2021-08-19 00:00:31+00:00  ...          0.7845\n",
              "43358  2021-08-19 00:00:01+00:00  ...          0.7783\n",
              "\n",
              "[43359 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sb2yl_zv_MU6"
      },
      "source": [
        "Cleaning up - NLP "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWvPPRDq0pCr",
        "outputId": "6088806a-bea1-4540-f0a3-79341bf569c5"
      },
      "source": [
        "!pip install unidecode\n",
        "!pip install word2number\n",
        "!pip install contractions\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import spacy\n",
        "\n",
        "import unidecode\n",
        "from word2number import w2n\n",
        "import contractions\n",
        "import nltk\n",
        "\n",
        "import string #punctuation removal\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop = stopwords.words('english')                      #English stopwords \n",
        "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "\n",
        "def remove_accented_chars(text):\n",
        "    \"\"\"remove accented characters from text, e.g. café\"\"\"\n",
        "    text = unidecode.unidecode(text)\n",
        "    return text\n",
        "\n",
        "def expand_contractions(text):\n",
        "    \"\"\"expand shortened words, e.g. don't to do not\"\"\"\n",
        "    text = contractions.fix(text)\n",
        "    return text\n",
        "\n",
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags \n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "for w in nlp.Defaults.stop_words:          ##Append stopwords from spacy into nltk's stopwords list \n",
        "  if w not in stop:\n",
        "    stop.append(w)\n",
        "\n",
        "stop.append(\"&amp\")    #Common redundant word\n",
        "\n",
        "def remove_user_ticker(text):\n",
        "  return re.subn('[@$]\\w+', '', text)[0].strip()    ## Remove all the @someone and $GMT whatever \n",
        "\n",
        "def remove_links(text):\n",
        "  return re.sub(r'http\\S+', '', text)         ## Remove all the twitter references https://t.co/somehting\n",
        "\n",
        "def remove_symbols_punctuation(text):      ## Removing all the symbols and punctuations in text --> mainly # and ! , . \n",
        "  return text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "def remove_digits(text):   ## Remove any number present in text on its own --> likely $123 became 123 after removing symbols\n",
        "    sen = []\n",
        "    for w in text.split():\n",
        "        if not w.isdigit():\n",
        "            sen.append(w)\n",
        "    return ' '.join(sen)\n",
        "\n",
        "def spacy_lemmatizer(text):    ## Lemmatize the words back to original form instead of stemming\n",
        "  sen = []\n",
        "  for w in nlp(text):\n",
        "    if w.lemma_ != \"-PRON-\":    ## spacy changes my etc to -PRON- (i.e pronoun) useless so remove\n",
        "     sen.append(w.lemma_)\n",
        "  return \" \".join(sen)\n",
        "\n",
        "def remove_single_char(text):   ## Empirically single & double character words (i, to, is) I think don't add much sentiment value so i removed\n",
        "  sen = []\n",
        "  for w in text.split():\n",
        "    if len(w) > 2:\n",
        "      sen.append(w)\n",
        "  return \" \".join(sen)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.2-py3-none-any.whl (235 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 19.3 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 20 kB 25.8 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30 kB 30.0 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 40 kB 30.5 MB/s eta 0:00:01\r\u001b[K     |███████                         | 51 kB 33.3 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 61 kB 36.2 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 71 kB 26.1 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 81 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 92 kB 28.4 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 102 kB 28.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 112 kB 28.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 122 kB 28.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 133 kB 28.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 143 kB 28.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 153 kB 28.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 163 kB 28.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 174 kB 28.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 184 kB 28.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 194 kB 28.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 204 kB 28.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 215 kB 28.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 225 kB 28.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 235 kB 28.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 235 kB 28.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.2\n",
            "Collecting word2number\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "Building wheels for collected packages: word2number\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5580 sha256=cc0857499558528be280065411d7c3ebe1c34759248682e6ea115b014a0020e7\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/c3/77/a5f48aeb0d3efb7cd5ad61cbd3da30bbf9ffc9662b07c9f879\n",
            "Successfully built word2number\n",
            "Installing collected packages: word2number\n",
            "Successfully installed word2number-1.1\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.0.52-py2.py3-none-any.whl (7.2 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.2.tar.gz (321 kB)\n",
            "\u001b[K     |████████████████████████████████| 321 kB 30.6 MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.3.0-py3-none-any.whl (284 kB)\n",
            "\u001b[K     |████████████████████████████████| 284 kB 40.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85451 sha256=d7d4c7908da824058b655cd19e65d2274e089cfdbf3fe64185a88ff31110ca12\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/19/a6/8f363d9939162782bb8439d886469756271abc01f76fbd790f\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.0 contractions-0.0.52 pyahocorasick-1.4.2 textsearch-0.0.21\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w-GC2cZ2pyK"
      },
      "source": [
        "def text_cleaner(df, content):\n",
        "  df[content] = df[content].apply(lambda s: remove_accented_chars(s))\n",
        "  df[content] = df[content].apply(lambda s: expand_contractions(s))\n",
        "  df[content] = df[content].apply(lambda s: remove_emoji(s))\n",
        "  df[content] = df[content].apply(lambda x: \" \".join(x.lower() for x in x.split() if x not in stop))\n",
        "  df[content] = df[content].apply(lambda s: remove_user_ticker(s))\n",
        "  df[content] = df[content].apply(lambda s: remove_links(s))\n",
        "  df[content] = df[content].apply(lambda s: remove_symbols_punctuation(s))\n",
        "  df[content] = df[content].apply(lambda s: remove_digits(s))\n",
        "  df[content] = df[content].apply(lambda s: spacy_lemmatizer(s))\n",
        "  df[content] = df[content].apply(lambda s: remove_single_char(s))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JD6zoboKMi-O",
        "outputId": "9968810b-85e1-43a4-8472-e4eb837e94a7"
      },
      "source": [
        "text_cleaner(amc2, 'content')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        }
      ]
    }
  ]
}